{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I present a simple rule for calculating derivatives for the backward propagation of all kinds of neural networks. I first derive the rule, then apply it to derive the backward propagation for different neural networks, such as plain neural networks, LSTM and CNN. Although the math of the rule is no different from the derivations you saw in anywhere else (I'm not inventing new maths of course), the main point I'd like to share is a simple way to get the backward propagation equations given a forward propagation equation. \n",
    "\n",
    "I know in practice most of the time you'll only need to provide the forward propagation and the frameworks will do the backward propagation for you, but I'm still interested in getting through all pieces of the neural networks, so I share some great tricks that allow you to derive the backward propagations quickly.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Rule For All Neural Networks\n",
    "\n",
    "Although the backward propagation only requires basic calculus, it still got some work there, especially for calculus with matrices. And after working the math out, we tend to forget how we get there. On the other day, we'll need to do the math again to get the results (if we had to do it by our own). However, in neural networks, actually only a few types of matrix calculus are involved, that's because we almost always use a linear layer and a non-linear layer. What makes it even simpler is, we almost always want to change the dimension in the linear layer (the layer's number of input neurons and output neurons are different) and keep the dimension in the non-linear layer (the layer's number of input neurons and output neurons are the same). That's interesting! It means we only need to find out the backward propagation for the following two types of forward propagation: \n",
    "$$\n",
    "\\begin{align}\n",
    "Z &= WX + b \\\\\n",
    "Z &= f(X)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where $X$ is the input, $Z$ is the output, $W$ is the weight matrix and $b$ is the bias, and $f$ is a non-linear function acting on $X$ element-wisely. Now we look at the two cases in detail. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. When the forward propagation is: $ Z = WX + b $\n",
    "\n",
    "While the dimension of $X$ can be anything (usually at least 2-D), for simplicity, we first consider a simple case where $X$ is two dimensional. Let's say $X$'s shape is $(N, M)$, $Z$'s shape is $(N^\\prime, M)$, $W$'s shape is $(N^\\prime, N)$, $b$'s shape is $(1, M)$. We have forward propagation of this form when the input layer has $N$ neurons and the output layer has $N^\\prime$ neurons, and we have $M$ samples in total.  \n",
    "\n",
    "For the backward propagation, we need to figure out the derivatives for $X$ , $W$, and $b$, in terms of the derivatives for $Z$. The derivatives for $X$ will be used for propagating further back; the derivatives for $W$ and $b$ will be used for updating these two parameters during training. \n",
    "\n",
    "By \"derivatives for $Z$\", I mean we want to get the derivatives for every element of $Z$: ${\\partial J/\\partial Z_{ij}}$, where $J$ is the objective to optimize. We can use derivatives for matrix to simplify our notation. Define: \n",
    "\n",
    "$$\\big({\\partial J\\over\\partial Z}\\big)_{ij} := {\\partial J\\over\\partial Z_{ij}}$$\n",
    "\n",
    "Then we can use $\\partial J/\\partial Z$, a matrix, to represent all the derivative terms.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Derivatives for $X$\n",
    "\n",
    "Our goal here is to derive ${\\partial J/\\partial X}$ given ${\\partial J/\\partial Z}$. By the chain rule, we have: \n",
    "\n",
    "$${\\partial J\\over\\partial X} = {\\partial J\\over\\partial Z}{\\partial Z\\over\\partial X}$$\n",
    "\n",
    "First, we calculate ${\\partial Z/\\partial X}$. This derivative is matrix respect to matrix, we may have a different mathematical definition for general cases, however, in here, I'd like to define it as: \n",
    "\n",
    "$$\\big({\\partial Z\\over\\partial X}\\big)_{ijm} = {\\partial Z_{im}\\over\\partial X_{jm}}$$\n",
    "\n",
    "Why? Because the last dimension of $X$ and $Z$ are the input samples, when we train the model, we do it sample by sample. We're not interested in derivatives across different samples. \n",
    "\n",
    "With this definition, now we can apply normal calculus:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\big({\\partial Z\\over\\partial X}\\big)_{ijm} &= {\\partial Z_{im}\\over\\partial X_{jm}} \\\\\n",
    "&= {\\partial \\big(\\sum_k W_{ik} X_{km} + b_i\\big)\\over\\partial X_{jm}} \\\\\n",
    "&= W_{ij}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Then use the chain rule:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\big({\\partial J\\over\\partial X}\\big)_{im} &= \\big({\\partial J\\over\\partial Z} {\\partial Z\\over\\partial X}\\big)_{ijm} \\\\\n",
    "&= \\sum_j \\big({\\partial J\\over\\partial Z}\\big)_{jm}\\big({\\partial Z\\over\\partial X}\\big)_{jim} \\\\\n",
    "&= \\sum_j W_{ji} \\big({\\partial J\\over\\partial Z}\\big)_{jm}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note if $A_{im} = \\sum_j B_{ji} C_{jm}$, then $A=B^T C$. So we have:\n",
    "\n",
    "$$\n",
    "{\\partial J\\over\\partial X} = W^T {\\partial J\\over\\partial Z}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Derivatives for W\n",
    "\n",
    "Here we want to derive ${\\partial J/\\partial W}$ given ${\\partial J/\\partial Z}$. By the chain rule, we have: \n",
    "\n",
    "$${\\partial J\\over\\partial W} = {\\partial J\\over\\partial Z}{\\partial Z\\over\\partial W}$$\n",
    "\n",
    "First, we calculate ${\\partial Z/\\partial W}$. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\big({\\partial Z\\over\\partial W}\\big)_{kmij} &= {\\partial Z_{km}\\over\\partial W_{ij}} \\\\\n",
    "&= {\\partial \\big(\\sum_n W_{kn} X_{nm} + b_k\\big)\\over\\partial W_{ij}} \\\\\n",
    "&= \\delta_{ki}X_{jm}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where $\\delta_{ij} = 1$ if $i = j$, otherwise $\\delta_{ij} = 0$. Now use the chain rule:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\big({\\partial J\\over\\partial W}\\big)_{ij} &= \\big({\\partial J\\over\\partial Z} {\\partial Z\\over\\partial W}\\big)_{ij} \\\\\n",
    "&= \\sum_{km} \\big({\\partial J\\over\\partial Z}\\big)_{km}\\big({\\partial Z\\over\\partial W}\\big)_{kmij} \\\\\n",
    "&= \\sum_{km} \\big({\\partial J\\over\\partial Z}\\big)_{km}\\delta_{ki}X_{jm} \\\\\n",
    "&= \\sum_m \\big({\\partial J\\over\\partial Z}\\big)_{im} X_{jm}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The sum of the last dimension means a sum over all samples. Note if $A_{ij} = \\sum_j B_{im} C_{jm}$, then $A=B C^T$. So: \n",
    "\n",
    "$$\n",
    "{\\partial J\\over\\partial W} = {\\partial J\\over\\partial Z} X^T\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Derivatives for b\n",
    "\n",
    "In here we'd like to derive ${\\partial J/\\partial b}$ given ${\\partial J/\\partial Z}$. By the chain rule, we have: \n",
    "\n",
    "$${\\partial J\\over\\partial b} = {\\partial J\\over\\partial Z}{\\partial Z\\over\\partial b}$$\n",
    "\n",
    "First, we calculate ${\\partial Z/\\partial b}$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\big({\\partial Z\\over\\partial b}\\big)_{ijm} &= {\\partial Z_{im}\\over\\partial b_{j}} \\\\\n",
    "&= {\\partial \\big(\\sum_k W_{ik} X_{km} + b_i\\big)\\over\\partial b_{j}} \\\\\n",
    "&= \\delta_{ij}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Apply the chain rule:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\big({\\partial J\\over\\partial b}\\big)_{i} &= \\big({\\partial J\\over\\partial Z} {\\partial Z\\over\\partial b}\\big)_{i} \\\\\n",
    "&= \\sum_{jm} \\big({\\partial J\\over\\partial Z}\\big)_{jm}\\big({\\partial Z\\over\\partial b}\\big)_{jim} \\\\\n",
    "&= \\sum_{jm} \\delta_{ij}\\big({\\partial J\\over\\partial Z}\\big)_{jm} \\\\\n",
    "&= \\sum_m \\big({\\partial J\\over\\partial Z}\\big)_{im}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Again, the sum of the last dimension means a sum over all samples. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Summary\n",
    "\n",
    "In summary, if the forward propagation is in the form of:\n",
    "$$ Z = W X + b$$\n",
    "\n",
    "Then:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "{\\partial J\\over\\partial X} &= W^T {\\partial J\\over\\partial Z} \\\\\n",
    "{\\partial J\\over\\partial W} &= {\\partial J\\over\\partial Z} X^T \\\\\n",
    "\\big({\\partial J\\over\\partial b}\\big)_{i} &= \\sum_m \\big({\\partial J\\over\\partial Z}\\big)_{im}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. When the forward propagation is: $ A = f(Z) $\n",
    "\n",
    "The forward propagation is this form when we apply activations. Usually $f$ is a non-linear funtion, and it's applied to every element of $Z$. So for the derivatives, we can calculate element-wisely. The derivatives calculation is then pretty straightforward: \n",
    "\n",
    "$${\\partial J\\over\\partial Z} = {\\partial J\\over\\partial A} * f^\\prime(Z) $$\n",
    "\n",
    "\"$*$\" means element-wise multiplication. $f^\\prime(Z)$ has the same dimension as $A$ and $Z$, so it's valid to perform element-wise multiplication. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at some most commonly used non-linear functions. \n",
    "\n",
    "#### 2.1 Sigmoid\n",
    "\n",
    "The derivative for the sigmoid function is: \n",
    "\n",
    "$$\\sigma^\\prime(X) = \\sigma(X) * (1 - \\sigma(X))$$\n",
    "\n",
    "Then we have: \n",
    "\n",
    "$${\\partial J\\over\\partial Z} = {\\partial J\\over\\partial A} * A * (1 - A) $$\n",
    "\n",
    "#### 2.2 Tanh\n",
    "\n",
    "The derivative for the $\\tanh$ function is: \n",
    "\n",
    "$$\\tanh^\\prime(X) = 1 - \\tanh(X)^2$$\n",
    "\n",
    "Then we have: \n",
    "\n",
    "$${\\partial J\\over\\partial Z} = {\\partial J\\over\\partial A} * (1 - A^2) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Summary\n",
    "\n",
    "Another notation is often used for the partial derivatives against the objective function:\n",
    "$$dX := {\\partial J\\over\\partial X}$$\n",
    "\n",
    "I don't quite like this notation because $dX$ has different meanings in math and it's sometimes confusing. But since this notation is widely used already and is more concise, I'll summarize our rules using this notation:\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"border: 2px solid #A4A4A4; padding: 5px\">\n",
    "**Rule 1**: <br />\n",
    "Forward propagation: $$Z = WX + b$$\n",
    "$$\\Downarrow$$\n",
    "Backward propagation:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "dX &= W^T dZ \\\\\n",
    "dW &= dZ \\, X^T \\\\\n",
    "\\big(db\\big)_{i} &= \\sum_m \\big(dZ\\big)_{im}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Rule 2**: <br />\n",
    "Forward propagation: $$A = f(Z)$$\n",
    "$$\\Downarrow$$\n",
    "Backward propagation:\n",
    "$$dZ = dA * f^\\prime(Z) $$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For rule 1, just put $dZ$ on the right hand side and multiply a transposed matrix, to match the dimension. Rule 2 is pretty straightforward, there's no change in dimension. Isn't it simple? \n",
    "\n",
    "Great, now let's use this to derive the backward propagation for different types of neural networks. You'll see the power of the simple rules when we come to complicated forward propagations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application:  A Simple Example\n",
    "\n",
    "Now we apply the rules to derive the backward propagation for a simple example: a neural networks with a stack of fully connected (FC) layers. \n",
    "\n",
    "The forward propagation is:\n",
    "$$\n",
    "\\begin{align}\n",
    "a^{[1]} &= relu(W_{1} X + b_1)\\tag{s.1} \\\\\n",
    "a^{[2]} &= relu(W_{2} a^{[1]} + b_2)\\tag{s.2} \\\\\n",
    "y &= sigmoid(a^{[2]})\\tag{s.3}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$a^{[i]}$ means the $a$ of the $i$'th layer. \n",
    "\n",
    "For Eq. (s.3), by rule (2):\n",
    "\n",
    "$${\\partial J\\over\\partial a^{[2]}} = {\\partial J\\over\\partial y} * y * (1-y)$$\n",
    "\n",
    "The value of ${\\partial J/\\partial y}$ depends on the definition of the loss function $J(y, y^*)$, where $y$ is the model output and $y^*$ is the label of the training sample. \n",
    "\n",
    "For Eq. (s.2), let $Z^{[2]} = W_{2} a^{[1]} + b_2$, then $a^{[2]} = relu(Z^{[2]})$. By rule (1):\n",
    "$${\\partial J\\over\\partial Z^{[2]}} = {\\partial J\\over\\partial a^{[2]}} relu^\\prime(Z^{[2]})$$\n",
    "\n",
    "Then by rule (2):\n",
    "$$\n",
    "\\begin{align}\n",
    "{\\partial J\\over\\partial a^{[1]}} &= {\\partial J\\over\\partial a^{[2]}} relu^\\prime(Z^{[2]}) W_{2}^T a^{[2]} \\\\\n",
    "{\\partial J\\over\\partial W_{2}} &= {\\partial J\\over\\partial a^{[2]}} relu^\\prime(Z^{[2]}) a^{[2]} (a^{[1]})^T \\\\\n",
    "{\\partial J\\over\\partial b_{2}} &= {\\partial J\\over\\partial a^{[2]}} relu^\\prime(Z^{[2]}) \\sum_m a^{[2]} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Same for Eq. (s.1):\n",
    "$$\n",
    "\\begin{align}\n",
    "{\\partial J\\over\\partial X} &= {\\partial J\\over\\partial a^{[1]}} relu^\\prime(Z^{[1]}) W_{1}^T a^{[1]} \\\\\n",
    "{\\partial J\\over\\partial W_{1}} &= {\\partial J\\over\\partial a^{[1]}} relu^\\prime(Z^{[1]}) X (a^{[1]})^T \\\\\n",
    "{\\partial J\\over\\partial b_{2}} &= {\\partial J\\over\\partial a^{[1]}} relu^\\prime(Z^{[1]}) \\sum_m a^{[1]} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where $Z^{[1]} = W_{1} X + b_1$. You can easily generalize these equations to deeper neural networks. You can just replace $relu$ (and its derivative) if a different non-linear function is used for activation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: RNN\n",
    "\n",
    "### 1 Simple RNN\n",
    "\n",
    "A simple plain RNN unit takes $x^{\\langle t \\rangle} $ and $a^{\\langle t-1 \\rangle}$ as input at time $t$, and returns $a^{\\langle t \\rangle}$ as output. $a^{\\langle t \\rangle}$ is RNN cell's hidden state at time $t$. If the RNN has $n$ units, and we feed $m$ samples, the shape of $a^{\\langle t \\rangle}$ is $(m, n)$. The trainable parameters are $W_{aa}$, $W_{ax}$ and $b_a$. \n",
    "\n",
    "The forward propagation is: \n",
    "$$a^{\\langle t \\rangle} = \\tanh(W_{aa} a^{\\langle t-1 \\rangle} + W_{ax} x^{\\langle t \\rangle} + b_a)$$\n",
    "\n",
    "For this form matches the form of our rules, the only difference is instead of $WX + b$, we have $W_A A + W_X X + b$, but when taking derivatives of one variable, we can treat other variables as constants, so we can apply the rules directly to get the backward propagation: \n",
    "\n",
    "$${\\partial J\\over\\partial W_{aa}} = \\left(1 - (a^{\\langle t \\rangle})^2\\right){\\partial J \\over\\partial a^{\\langle t \\rangle}}\\left(a^{\\langle t - 1 \\rangle}\\right)^T$$\n",
    "$${\\partial J\\over\\partial a^{\\langle t - 1 \\rangle}} =  \\left(1 - (a^{\\langle t \\rangle})^2\\right) W_{aa}^T {\\partial J \\over\\partial a^{\\langle t \\rangle}}$$\n",
    "$${\\partial J\\over\\partial W_{ax}} = \\left(1 - (a^{\\langle t \\rangle})^2\\right){\\partial J \\over\\partial a^{\\langle t \\rangle}}\\left(x^{\\langle t \\rangle}\\right)^T$$\n",
    "$${\\partial J\\over\\partial x^{\\langle t \\rangle}} = \\left(1 - (a^{\\langle t \\rangle})^2\\right) W_{ax}^T{\\partial J \\over\\partial a^{\\langle t \\rangle}}$$\n",
    "$${\\partial J\\over\\partial b_a} =  \\sum \\left(1 - (a^{\\langle t \\rangle})^2\\right){\\partial J \\over\\partial a^{\\langle t \\rangle}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the ${\\partial J /\\partial a^{\\langle t-1 \\rangle}}$ in here just contains the gradients from the next cell (at time $t$); we also have gradients from this cell's output $\\hat{y}^{\\langle t-1 \\rangle}$, where: \n",
    "\n",
    "$$\\hat{y}^{\\langle t-1 \\rangle} = sigmoid(W_{ya} a^{\\langle t-1 \\rangle} + b_y)$$\n",
    "\n",
    "Of course the activation can also be other non-linearities (e.g., softmax). By our rules, we have:\n",
    "$${\\partial J\\over\\partial a^{\\langle t - 1 \\rangle}} = \\hat{y}^{\\langle t - 1 \\rangle} * \\left(1-\\hat{y}^{\\langle t - 1 \\rangle}\\right) * W_{ya}^T{\\partial J\\over\\partial \\hat{y}^{\\langle t - 1 \\rangle}}$$\n",
    "\n",
    "Combining the contributions from both next cell and the output of the current cell, we have:  \n",
    "$${\\partial J\\over\\partial a^{\\langle t - 1 \\rangle}} = \\left(1 - (a^{\\langle t \\rangle})^2\\right) W_{aa}^T {\\partial J \\over\\partial a^{\\langle t \\rangle}} + \\hat{y}^{\\langle t - 1 \\rangle} * \\left(1-\\hat{y}^{\\langle t - 1 \\rangle}\\right) *W_{ya}^T{\\partial J\\over\\partial \\hat{y}^{\\langle t - 1 \\rangle}}$$\n",
    "\n",
    "As a side note, we can also write the RNN forward propagation in a compact form: \n",
    "$$a^{\\langle t \\rangle} = \\tanh(\\begin{bmatrix} W_{aa} W_{ax} \\end{bmatrix} \\begin{bmatrix} a^{\\langle t-1 \\rangle} \\\\ x^{\\langle t \\rangle}\\end{bmatrix} + b_a)$$\n",
    "$$\\hat{y}^{\\langle t \\rangle} = sigmoid(W_{ya} a^{\\langle t \\rangle} + b_y)$$\n",
    "\n",
    "Then it's even more straightforward to apply our rules. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 LSTM\n",
    "\n",
    "A LSTM cell takes $x^{\\langle t \\rangle}$, $a^{\\langle t-1 \\rangle}$ and $c^{\\langle t-1 \\rangle}$ as inputs, returns $a^{\\langle t \\rangle}$, $c^{\\langle t \\rangle}$ as outputs, at time $t$. In here, $a^{\\langle t \\rangle}$ is the hidden state, $c^{\\langle t \\rangle}$ is the cell state, they have the same dimension. If the LSTM has $n$ units, and we feed $m$ samples, the shape of $a^{\\langle t \\rangle}$ is $(m, n)$. \n",
    "\n",
    "The forward propagation is:\n",
    "\n",
    "$$\\Gamma_u^{\\langle t \\rangle} = \\sigma\\left(W_u\\begin{bmatrix} a^{\\langle t-1 \\rangle} \\\\ x^{\\langle t \\rangle}\\end{bmatrix} + b_u \\right)\\tag{r.2.1}$$\n",
    "$$\\Gamma_f^{\\langle t \\rangle} = \\sigma\\left(W_f\\begin{bmatrix} a^{\\langle t-1 \\rangle} \\\\ x^{\\langle t \\rangle}\\end{bmatrix} + b_f \\right)\\tag{r.2.2}$$\n",
    "$$\\Gamma_o^{\\langle t \\rangle} = \\sigma\\left(W_o\\begin{bmatrix} a^{\\langle t-1 \\rangle} \\\\ x^{\\langle t \\rangle}\\end{bmatrix} + b_o \\right)\\tag{r.2.3}$$\n",
    "$$\\widetilde{c}^{\\langle t \\rangle} = \\tanh\\left(W_c\\begin{bmatrix} a^{\\langle t-1 \\rangle} \\\\ x^{\\langle t \\rangle}\\end{bmatrix} + b_c \\right)\\tag{r.2.4}$$\n",
    "\n",
    "$$c^{\\langle t \\rangle} = \\Gamma_u^{\\langle t \\rangle} * \\widetilde{c}^{\\langle t \\rangle} + \\Gamma_f^{\\langle t \\rangle} * c^{\\langle t-1 \\rangle}\\tag{r.2.5}$$\n",
    "$$a^{\\langle t \\rangle} = \\Gamma_o^{\\langle t \\rangle} * \\tanh\\left(c^{\\langle t \\rangle}\\right)\\tag{r.2.6}$$\n",
    "\n",
    "Where $\\Gamma_u^{\\langle t \\rangle}$ is the update gate, $\\Gamma_f^{\\langle t \\rangle}$ is the forget gate.  If $\\Gamma_u^{\\langle t \\rangle} = 0$ and $\\Gamma_f^{\\langle t \\rangle} = 1$, then the new cell state equals to the previous cell state; If $\\Gamma_u^{\\langle t \\rangle} = 1$ and $\\Gamma_f^{\\langle t \\rangle} = 0$, then the previous cell state will be completely forgotten. \n",
    "$\\Gamma_o^{\\langle t \\rangle}$ is the output gate, this gate multiplies by the cell state gives the new hidden state. \n",
    "All the weight matrices $W$ and bias vectors $b$ are trainable parameters. The gate notations are from Andrew Ng. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 The update gate\n",
    "We first look at $W_u$ and $b_u$. From Eq. (r.2.1), our rules can be applied directly to get: \n",
    "\n",
    "$${\\partial J\\over\\partial W_{u}} = {\\partial J\\over\\partial \\Gamma_u^{\\langle t \\rangle}}\\begin{bmatrix} a^{\\langle t-1 \\rangle} \\\\ x^{\\langle t \\rangle}\\end{bmatrix}^T * \\Gamma_u^{\\langle t \\rangle} * \\big(1-\\Gamma_u^{\\langle t \\rangle}\\big)\\tag{r.2.7}$$\n",
    "\n",
    "We need to use ${\\partial J/\\partial a^{\\langle t \\rangle}}$ and ${\\partial J/\\partial c^{\\langle t \\rangle}}$ to figure out ${\\partial J/\\partial \\Gamma_u^{\\langle t \\rangle}}$, because the outputs are $a^{\\langle t \\rangle}$ and $c^{\\langle t \\rangle}$, and we're propagating backwards. \n",
    "\n",
    "From Eq. (r.2.5), we have: \n",
    "$${\\partial J\\over\\partial \\Gamma_u^{\\langle t \\rangle}} = {\\partial J\\over\\partial c^{\\langle t \\rangle}} * \\widetilde{c}^{\\langle t \\rangle}$$\n",
    "\n",
    "Here we need to be careful, because not only ${\\partial J/\\partial c^{\\langle t \\rangle}}$ has contribution to ${\\partial J/\\partial \\Gamma_u^{\\langle t \\rangle}}$, ${\\partial J/\\partial a^{\\langle t \\rangle}}$ also does. As we can see from Eq. (r.2.6), we have: \n",
    "$$\n",
    "{\\partial J\\over\\partial c^{\\langle t \\rangle}} = {\\partial J\\over\\partial a^{\\langle t \\rangle}} * \\Gamma_o^{\\langle t \\rangle} *\\big(1-\\tanh(c^{\\langle t \\rangle})^2\\big)\n",
    "$$\n",
    "\n",
    "Sum it up:\n",
    "$${\\partial J\\over\\partial \\Gamma_u^{\\langle t \\rangle}} = \\big({\\partial J\\over\\partial c^{\\langle t \\rangle}} + {\\partial J\\over\\partial a^{\\langle t \\rangle}} * \\Gamma_o^{\\langle t \\rangle} *\\big(1-\\tanh(c^{\\langle t \\rangle})^2\\big)\\big) * \\widetilde{c}^{\\langle t \\rangle}\\tag{r.2.8}$$\n",
    "\n",
    "We can then plug it to Eq. (r.2.7) to get ${\\partial J/\\partial W_{u}}$.\n",
    "\n",
    "By our rules, it's easy to get:\n",
    "\n",
    "$${\\partial J\\over\\partial b_{u}} = \\sum_m {\\partial J\\over\\partial \\Gamma_u^{\\langle t \\rangle}} * \\Gamma_u^{\\langle t \\rangle} * \\big(1-\\Gamma_u^{\\langle t \\rangle}\\big)$$\n",
    "\n",
    "Where ${\\partial J/\\partial \\Gamma_u^{\\langle t \\rangle}}$ is in Eq. (r.2.8). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 The forget gate\n",
    "Now we look at $W_f$ and $b_f$, the process will be very similar to the update gate. From Eq. (r.2.2), our rules can be applied directly to get: \n",
    "\n",
    "$${\\partial J\\over\\partial W_{f}} = {\\partial J\\over\\partial \\Gamma_f^{\\langle t \\rangle}}\\begin{bmatrix} a^{\\langle t-1 \\rangle} \\\\ x^{\\langle t \\rangle}\\end{bmatrix}^T * \\Gamma_f^{\\langle t \\rangle} * \\big(1-\\Gamma_f^{\\langle t \\rangle}\\big)\\tag{r.2.9}$$\n",
    "\n",
    "From Eq. (r.2.5), we have: \n",
    "$${\\partial J\\over\\partial \\Gamma_f^{\\langle t \\rangle}} = {\\partial J\\over\\partial c^{\\langle t \\rangle}} * c^{\\langle t-1 \\rangle}$$\n",
    "\n",
    "Similarly, we also have contribution from Eq. (r.2.6): \n",
    "$$\n",
    "{\\partial J\\over\\partial c^{\\langle t \\rangle}} = {\\partial J\\over\\partial a^{\\langle t \\rangle}} * \\Gamma_o^{\\langle t \\rangle} *\\big(1-\\tanh(c^{\\langle t \\rangle})^2\\big)\n",
    "$$\n",
    "\n",
    "So finally:\n",
    "$${\\partial J\\over\\partial \\Gamma_f^{\\langle t \\rangle}} = \\big({\\partial J\\over\\partial c^{\\langle t \\rangle}} + {\\partial J\\over\\partial a^{\\langle t \\rangle}} * \\Gamma_o^{\\langle t \\rangle} *\\big(1-\\tanh(c^{\\langle t \\rangle})^2\\big)\\big) * c^{\\langle t-1 \\rangle}\\tag{r.2.10}$$\n",
    "\n",
    "We can then plug it to Eq. (r.2.9) to get ${\\partial J/\\partial W_{f}}$.\n",
    "\n",
    "For ${\\partial J/\\partial b_{f}}$:\n",
    "\n",
    "$${\\partial J\\over\\partial b_{f}} = \\sum_m {\\partial J\\over\\partial \\Gamma_f^{\\langle t \\rangle}} * \\Gamma_f^{\\langle t \\rangle} * \\big(1-\\Gamma_f^{\\langle t \\rangle}\\big)$$\n",
    "\n",
    "Where ${\\partial J/\\partial \\Gamma_f^{\\langle t \\rangle}}$ is in Eq. (r.2.10). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 The output gate\n",
    "\n",
    "This one is easy, because it's not in Eq. (r.2.5). From Eq. (r.2.3): \n",
    "\n",
    "$${\\partial J\\over\\partial W_{o}} = {\\partial J\\over\\partial \\Gamma_o^{\\langle t \\rangle}}\\begin{bmatrix} a^{\\langle t-1 \\rangle} \\\\ x^{\\langle t \\rangle}\\end{bmatrix}^T * \\Gamma_o^{\\langle t \\rangle} * \\big(1-\\Gamma_o^{\\langle t \\rangle}\\big)$$\n",
    "\n",
    "From Eq. (r.2.6): \n",
    "$$\n",
    "{\\partial J\\over\\partial \\Gamma_o^{\\langle t \\rangle}} = {\\partial J\\over\\partial a^{\\langle t \\rangle}} * \\tanh(c^{\\langle t \\rangle})\\tag{r.2.11}\n",
    "$$\n",
    "\n",
    "So:\n",
    "$${\\partial J\\over\\partial W_{o}} = {\\partial J\\over\\partial a^{\\langle t \\rangle}} * \\tanh(c^{\\langle t \\rangle}) * \\Gamma_o^{\\langle t \\rangle} * \\big(1-\\Gamma_o^{\\langle t \\rangle}\\big) \\begin{bmatrix} a^{\\langle t-1 \\rangle} \\\\ x^{\\langle t \\rangle}\\end{bmatrix}^T$$\n",
    "\n",
    "Then:\n",
    "$${\\partial J\\over\\partial b_{o}} = \\sum_m{\\partial J\\over\\partial a^{\\langle t \\rangle}} * \\tanh(c^{\\langle t \\rangle}) * \\Gamma_o^{\\langle t \\rangle} * \\big(1-\\Gamma_o^{\\langle t \\rangle}\\big) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 The candidate cell state\n",
    "\n",
    "We're almost there! From Eq. (r.2.4): \n",
    "\n",
    "$${\\partial J\\over\\partial W_c} = {\\partial J\\over\\partial \\widetilde{c}^{\\langle t \\rangle}}\\begin{bmatrix} a^{\\langle t-1 \\rangle} \\\\ x^{\\langle t \\rangle}\\end{bmatrix}^T * \\left(1- \\left(\\widetilde{c}^{\\langle t \\rangle}\\right)^2\\right)$$\n",
    "\n",
    "From Eq. (r.2.5), we have: \n",
    "$${\\partial J\\over\\partial \\widetilde{c}^{\\langle t \\rangle}} = {\\partial J\\over\\partial c^{\\langle t \\rangle}} * \\Gamma_u^{\\langle t \\rangle}$$\n",
    "\n",
    "Similarly, we also have contribution from Eq. (r.2.6): \n",
    "$$\n",
    "{\\partial J\\over\\partial c^{\\langle t \\rangle}} = {\\partial J\\over\\partial a^{\\langle t \\rangle}} * \\Gamma_o^{\\langle t \\rangle} *\\big(1-\\tanh(c^{\\langle t \\rangle})^2\\big)\n",
    "$$\n",
    "\n",
    "Sum it up:\n",
    "$${\\partial J\\over\\partial \\widetilde{c}^{\\langle t \\rangle}} = \\left({\\partial J\\over\\partial c^{\\langle t \\rangle}} + {\\partial J\\over\\partial a^{\\langle t \\rangle}} * \\Gamma_o^{\\langle t \\rangle} *\\left(1-\\tanh(c^{\\langle t \\rangle})^2\\right)\\right) * \\Gamma_u^{\\langle t \\rangle}\\tag{r.2.12}$$\n",
    "\n",
    "For ${\\partial J/\\partial b_{c}}$:\n",
    "\n",
    "$${\\partial J\\over\\partial b_{c}} = \\sum_m {\\partial J\\over\\partial \\widetilde{c}^{\\langle t \\rangle}}* \\left(1- \\left(\\widetilde{c}^{\\langle t \\rangle}\\right)^2\\right)$$\n",
    "\n",
    "Where ${\\partial J/\\partial \\widetilde{c}^{\\langle t \\rangle}}$ is in Eq. (r.2.12). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Input, hidden state and cell state\n",
    "\n",
    "This is the final step! First we look at the derivatives for $a^{\\langle t-1 \\rangle}$ and $x^{\\langle t \\rangle}$. They always appear together in Eq. (r.2.1-4), so we have: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "{\\partial J\\over\\partial\\begin{bmatrix} a^{\\langle t-1 \\rangle} \\\\ x^{\\langle t \\rangle}\\end{bmatrix}} = & W_u^T {\\partial J\\over\\partial \\Gamma_u^{\\langle t \\rangle}} * \\Gamma_u^{\\langle t \\rangle} * \\big(1-\\Gamma_u^{\\langle t \\rangle}\\big) \\\\\n",
    "& + W_f^T {\\partial J\\over\\partial \\Gamma_f^{\\langle t \\rangle}} * \\Gamma_f^{\\langle t \\rangle} * \\big(1-\\Gamma_f^{\\langle t \\rangle}\\big) \\\\\n",
    "& + W_o^T {\\partial J\\over\\partial \\Gamma_o^{\\langle t \\rangle}} * \\Gamma_o^{\\langle t \\rangle} * \\big(1-\\Gamma_o^{\\langle t \\rangle}\\big) \\\\\n",
    "& + W_c^T{\\partial J\\over\\partial \\widetilde{c}^{\\langle t \\rangle}}* \\left(1- \\left(\\widetilde{c}^{\\langle t \\rangle}\\right)^2\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The partial derivatives on the right hand side are already calculated in Eq. (2.1.8) (2.1.10) (2.1.11) (2.1.12).\n",
    "\n",
    "Now we calculate the derivative for $c^{\\langle t-1 \\rangle}$. From Eq. (r.2.5), we have:\n",
    "$$\n",
    "{\\partial J\\over\\partial c^{\\langle t-1 \\rangle}} = {\\partial J\\over\\partial c^{\\langle t \\rangle}} * \\Gamma_f^{\\langle t \\rangle}\n",
    "$$\n",
    "\n",
    "Similar to previous sections, $a^{\\langle t \\rangle}$ contributes to ${\\partial J/\\partial c^{\\langle t \\rangle}}$ as well, by Eq. (r.2.6). And:\n",
    "\n",
    "$$\n",
    "{\\partial J\\over\\partial c^{\\langle t \\rangle}} = {\\partial J\\over\\partial a^{\\langle t \\rangle}} * \\Gamma_o^{\\langle t \\rangle} *\\big(1-\\tanh(c^{\\langle t \\rangle})^2\\big)\n",
    "$$\n",
    "\n",
    "Sum the two terms up:\n",
    "$$\n",
    "{\\partial J\\over\\partial c^{\\langle t-1 \\rangle}} = \\left({\\partial J\\over\\partial c^{\\langle t \\rangle}} + {\\partial J\\over\\partial a^{\\langle t \\rangle}} * \\Gamma_o^{\\langle t \\rangle} *\\left(1-\\tanh(c^{\\langle t \\rangle})^2\\right) \\right)* \\Gamma_f^{\\langle t \\rangle}\n",
    "$$\n",
    "\n",
    "And that's it!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 LSTM backward propagation summary\n",
    "\n",
    "To sum up, the derivatives for the gates are:\n",
    "$$\n",
    "\\begin{align}\n",
    "{\\partial J\\over\\partial \\Gamma_u^{\\langle t \\rangle}} &= \\left({\\partial J\\over\\partial c^{\\langle t \\rangle}} + {\\partial J\\over\\partial a^{\\langle t \\rangle}} * \\Gamma_o^{\\langle t \\rangle} *\\left(1-\\tanh(c^{\\langle t \\rangle})^2\\right)\\right) * \\widetilde{c}^{\\langle t \\rangle} \\\\ \n",
    "{\\partial J\\over\\partial \\Gamma_f^{\\langle t \\rangle}} &= \\left({\\partial J\\over\\partial c^{\\langle t \\rangle}} + {\\partial J\\over\\partial a^{\\langle t \\rangle}} * \\Gamma_o^{\\langle t \\rangle} *\\left(1-\\tanh(c^{\\langle t \\rangle})^2\\right)\\right) * c^{\\langle t-1 \\rangle} \\\\\n",
    "{\\partial J\\over\\partial \\Gamma_o^{\\langle t \\rangle}} &= {\\partial J\\over\\partial a^{\\langle t \\rangle}} * \\tanh(c^{\\langle t \\rangle}) \\\\\n",
    "{\\partial J\\over\\partial \\widetilde{c}^{\\langle t \\rangle}} &= \\left({\\partial J\\over\\partial c^{\\langle t \\rangle}} + {\\partial J\\over\\partial a^{\\langle t \\rangle}} * \\Gamma_o^{\\langle t \\rangle} *\\left(1-\\tanh(c^{\\langle t \\rangle})^2\\right)\\right) * \\Gamma_u^{\\langle t \\rangle}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Derivatives for the $W$'s: \n",
    "$${\\partial J\\over\\partial W_{u}} = {\\partial J\\over\\partial \\Gamma_u^{\\langle t \\rangle}}\\begin{bmatrix} a^{\\langle t-1 \\rangle} \\\\ x^{\\langle t \\rangle}\\end{bmatrix}^T * \\Gamma_u^{\\langle t \\rangle} * \\big(1-\\Gamma_u^{\\langle t \\rangle}\\big)$$\n",
    "$${\\partial J\\over\\partial W_{f}} = {\\partial J\\over\\partial \\Gamma_f^{\\langle t \\rangle}}\\begin{bmatrix} a^{\\langle t-1 \\rangle} \\\\ x^{\\langle t \\rangle}\\end{bmatrix}^T * \\Gamma_f^{\\langle t \\rangle} * \\big(1-\\Gamma_f^{\\langle t \\rangle}\\big)$$\n",
    "$${\\partial J\\over\\partial W_{o}} = {\\partial J\\over\\partial \\Gamma_o^{\\langle t \\rangle}}\\begin{bmatrix} a^{\\langle t-1 \\rangle} \\\\ x^{\\langle t \\rangle}\\end{bmatrix}^T * \\Gamma_o^{\\langle t \\rangle} * \\big(1-\\Gamma_o^{\\langle t \\rangle}\\big)$$\n",
    "$${\\partial J\\over\\partial W_c} = {\\partial J\\over\\partial \\widetilde{c}^{\\langle t \\rangle}}\\begin{bmatrix} a^{\\langle t-1 \\rangle} \\\\ x^{\\langle t \\rangle}\\end{bmatrix}^T * \\left(1- \\left(\\widetilde{c}^{\\langle t \\rangle}\\right)^2\\right)$$\n",
    "\n",
    "Derivatives for the $b$'s: \n",
    "$$\n",
    "\\begin{align}\n",
    "{\\partial J\\over\\partial b_{u}} &= \\sum_m {\\partial J\\over\\partial \\Gamma_u^{\\langle t \\rangle}} * \\Gamma_u^{\\langle t \\rangle} * \\big(1-\\Gamma_u^{\\langle t \\rangle}\\big) \\\\\n",
    "{\\partial J\\over\\partial b_{f}} &= \\sum_m {\\partial J\\over\\partial \\Gamma_f^{\\langle t \\rangle}} * \\Gamma_f^{\\langle t \\rangle} * \\big(1-\\Gamma_f^{\\langle t \\rangle}\\big) \\\\\n",
    "{\\partial J\\over\\partial b_{o}} &= \\sum_m{\\partial J\\over\\partial \\Gamma_o^{\\langle t \\rangle}} * \\Gamma_o^{\\langle t \\rangle} * \\big(1-\\Gamma_o^{\\langle t \\rangle}\\big) \\\\\n",
    "{\\partial J\\over\\partial b_{c}} &= \\sum_m {\\partial J\\over\\partial \\widetilde{c}^{\\langle t \\rangle}}* \\left(1- \\left(\\widetilde{c}^{\\langle t \\rangle}\\right)^2\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Derivatives for the input and hidden state: \n",
    "$$\n",
    "\\begin{align}\n",
    "{\\partial J\\over\\partial\\begin{bmatrix} a^{\\langle t-1 \\rangle} \\\\ x^{\\langle t \\rangle}\\end{bmatrix}} = & W_u^T {\\partial J\\over\\partial \\Gamma_u^{\\langle t \\rangle}} * \\Gamma_u^{\\langle t \\rangle} * \\big(1-\\Gamma_u^{\\langle t \\rangle}\\big) \\\\\n",
    "& + W_f^T {\\partial J\\over\\partial \\Gamma_f^{\\langle t \\rangle}} * \\Gamma_f^{\\langle t \\rangle} * \\big(1-\\Gamma_f^{\\langle t \\rangle}\\big) \\\\\n",
    "& + W_o^T {\\partial J\\over\\partial \\Gamma_o^{\\langle t \\rangle}} * \\Gamma_o^{\\langle t \\rangle} * \\big(1-\\Gamma_o^{\\langle t \\rangle}\\big) \\\\\n",
    "& + W_c^T{\\partial J\\over\\partial \\widetilde{c}^{\\langle t \\rangle}}* \\left(1- \\left(\\widetilde{c}^{\\langle t \\rangle}\\right)^2\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Derivatives for the cell state: \n",
    "$$\n",
    "{\\partial J\\over\\partial c^{\\langle t-1 \\rangle}} = \\left({\\partial J\\over\\partial c^{\\langle t \\rangle}} + {\\partial J\\over\\partial a^{\\langle t \\rangle}} * \\Gamma_o^{\\langle t \\rangle} *\\left(1-\\tanh(c^{\\langle t \\rangle})^2\\right) \\right)* \\Gamma_f^{\\langle t \\rangle}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application:  CNN\n",
    "\n",
    "### 1 Convolution\n",
    "\n",
    "In each step of the convolution operation, we obtain one element of the next layer by calculating the sum of the direct multiplication of a part of the current layer and the filter. Let's say this part of the current layer is $A_{sub}^{[n]}$, the filter is $F$, then the forward propagation is:\n",
    "$$A^{[n + 1]}_{ij} = \\sum A_{sub}^{[n]} * F$$\n",
    "\n",
    "If we only look at this equation, the backward propagation is very straightforward: \n",
    "$$\n",
    "{\\partial J\\over\\partial A_{sub}^{[n]}} = {\\partial J\\over\\partial A^{[n + 1]}_{ij}} * F\n",
    "$$\n",
    "\n",
    "Note $A^{[n + 1]}_{ij}$ is a scalar, $A_{sub}^{[n]}$ and $F$ are matrices of the same shape. So applying this equation to the $(i, j)$'th element of $A^{[n + 1]}$ gives us the gradients of a part of $A^{[n]}$. And all convolutoin steps are independent, so we just need to loop through all $A_{sub}^{[n]}$'s elements and sum the results up! \n",
    "\n",
    "I can also write it in this form:\n",
    "\n",
    "$$\n",
    "{\\partial J\\over\\partial A^{[n]}} = \\sum_{i, j} {\\partial J\\over\\partial A^{[n + 1]}_{ij}} * M(i, j)\n",
    "$$\n",
    "\n",
    "Where $M(i, j)$ has the same shape of $A^{[n]}$. For the part of $A^{[n]}$ used to the convolution step for calculating $(i, j)$, $M(i, j)_{sub} = F$; outside of this part, values of $M(i, j)$ are all zeros. This means just to propagate the gradients back to where it comes from.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Max pooling\n",
    "\n",
    "The max pooling is very similar to the convolution, but instead of multiplying the filters and sum it, we just take the maximum element in the original layer as the element for the next layer. So the forward propagation is:\n",
    "$$A^{[n + 1]}_{ij} = \\max( A_{sub}^{[n]} )$$\n",
    "\n",
    "So the backward propagation is: \n",
    "$$\n",
    "{\\partial J\\over\\partial A_{i^\\prime j^\\prime}^{[n]}} = {\\partial J\\over\\partial A^{[n + 1]}_{ij}}\n",
    "$$\n",
    "\n",
    "Where $(i^\\prime, j^\\prime)$ is that maximum element of $A_{sub}^{[n]}$. In other words, you just need to assign the gradient from next layer to where it comes from in the previous layer, and accumulate. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Applications\n",
    "\n",
    "We can also to the same thing for the dropout layers, batch normalization, and Residue Neural Networks. I have derived truly marvelous results for all of them, which this notebook's margin is too narrow to contain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Afterword\n",
    "\n",
    "If you reach here and are still awake and reading, you're probably crazy, but of course I'm with you :). All the derivations here are not going to be very useful practically: even if you're implementing a new neural network architecture, unless you do everything by yourself, you'll probably only need to do the forward propagation, and never need to worry about the backward propagation. However the math is not that hard at all, if you go through it, the neural networks won't be a mysterious blackbox anymore. I hope it helps, thanks for reading! \n"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XaIWT",
   "launcher_item_id": "zAgPl"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
