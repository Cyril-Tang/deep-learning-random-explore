# An easy way to derive backward propagation for all kinds of neural networks

In practice most of the time you'll only need to provide the forward propagation and the frameworks will do the backward propagation for you, but in case you're interested in getting through all pieces of the neural networks by yourselves, I'd like to share some great tricks that allow you to derive the backward propagations quickly, for all kinds of neural networks (LSTM, CNN, etc.).

Check out this notebook `backward_propagation_for_all.ipynb` for all details. Please open it in Jupyter Notebook, the Github website doesn't render all equations correctly. 

Enjoy! 
